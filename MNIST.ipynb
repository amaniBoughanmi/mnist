{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1WoreyroLIX",
        "colab_type": "text"
      },
      "source": [
        "### **Part I**\n",
        "The objective is to build a classifier on MNIST dataset that meets certain performance criteria.\n",
        "For this task, you will need to build a test set with at least 1,000 samples per class.\n",
        "Train the model of your choice and compute the following metrics:\n",
        "1. Confusion matrix\n",
        "2. Per class FPR\n",
        "3. Per class TPR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn1mrKu1zASI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09a0afdd-2ef5-4a85-9e54-da93405650a2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, Activation, MaxPooling2D, AveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hozohTJ0J2b",
        "colab_type": "text"
      },
      "source": [
        "Below the definition of all needed functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvTZ4e-YE8p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_label_pred(proba_array):\n",
        "  \"\"\" this function get the position of the maximum value\n",
        "   of each row of the input proba_array.\n",
        "\n",
        "   Parameters\n",
        "   ----------\n",
        "    proba_array: 2D array\n",
        "    2 array in which each row contains \n",
        "    probabilities \n",
        "\n",
        "   Returns\n",
        "   -------\n",
        "   the position of the maximum value\n",
        "   of each row of the input proba_array.\n",
        "   \"\"\"\n",
        "  return np.argmax(proba_array, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ccTdDdKcH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_label_actual(one_hot_ecoder):\n",
        "  \"\"\"Giving an array of hot-enoder vectors\n",
        "  the function returns their label with length=1 \n",
        "\n",
        "  parameters\n",
        "  ----------\n",
        "  one_hot_ecoder: 2D array \n",
        "   an array containing one hot encoders\n",
        "   one hot encoder vector had this format\n",
        "   [[0,0,....,0,1,0]\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  label: int\n",
        "   the label accoding to the one_hot encoder vector\n",
        "  \"\"\"\n",
        "  label=np.array(one_hot_ecoder.idxmax(axis=1))\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr0MdYFkk5Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_class_rates(cnf_matrix):\n",
        "   \"\"\"This function compute metrics\n",
        "   from the confusion matrix;\n",
        "   true_positive, true_negative,\n",
        "   false positive, false_negative\n",
        "   true positive rate , false positive rate\n",
        "  \n",
        "   \n",
        "   Parameters\n",
        "   ----------\n",
        "   cnf_matrix: 2D array\n",
        "    confusion matrix\n",
        "   \n",
        "   Returns\n",
        "   -------\n",
        "   TPR: float\n",
        "    Sensitivity, hit rate, recall, or true positive rate\n",
        "   FPR: float\n",
        "    Fall out or false positive rate\n",
        "   \"\"\"\n",
        "\n",
        "  \n",
        "   FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "   FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "   TP = np.diag(cnf_matrix)\n",
        "   TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "   FP = FP.astype(float)\n",
        "   FN = FN.astype(float)\n",
        "   TP = TP.astype(float)\n",
        "   TN = TN.astype(float)\n",
        "\n",
        "   # Sensitivity, hit rate, recall, or true positive rate\n",
        "   TPR = TP/(TP+FN)\n",
        "\n",
        "   # Fall out or false positive rate\n",
        "   FPR = FP/(FP+TN)\n",
        "\n",
        "   return(TPR,FPR)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUjtxPxGsOlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_or_not_sure(vector, thresh):\n",
        "  \"\"\" This function return \"not sure\" if \n",
        "  the max of values in the vector dosen't\n",
        "  reach the threshold,\n",
        "  else return the index of the max value \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  vector: 1D array \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  label: int\n",
        "  the index of the max value \n",
        "  in the input vector\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  if (np.max(vector) < thresh):\n",
        "    return \"not_sure\"\n",
        "  else:\n",
        "    label = np.argmax(vector)\n",
        "    return(label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvt8L-1XUDhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def coverage(num_predicted_not_sure_examples,total_examples):\n",
        "  \"\"\" This function return the rate of uncertainty of the classifier which is\n",
        "   float between 0 and 1.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  num_predicted_not_sure_examples: int\n",
        "   the number of examples that are predicted as not sure\n",
        "\n",
        "  total_examples : int\n",
        "   the total number of examples\n",
        "\n",
        "   Returns\n",
        "   -------\n",
        "\n",
        "   cover: float\n",
        "    the rate of uncertainty of the classifier\n",
        "\n",
        "  \"\"\"\n",
        "  cover= 1.0-(num_predicted_not_sure_examples/total_examples)\n",
        "  return cover\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTWjI6BHDFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_with_coverage(examples_array,thresh):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  examples_array: 2D array \n",
        "   contain all predicted examples vectors\n",
        "\n",
        "  thresh: float\n",
        "   \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  cover: float\n",
        "   rate of certainty of the classifier\n",
        "   \n",
        "  \n",
        "  not_predict_indices: list\n",
        "   list of indices that are predicted as not sure\n",
        "  \n",
        "  y_predicted: list\n",
        "   predicted label with certainty\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  num_predicted_not_sure=0.0\n",
        "  not_predict_indices=[]\n",
        "  y_predicted=[]\n",
        "\n",
        "  for i in range(len(examples_array)):\n",
        "  \n",
        "    if label_or_not_sure(examples_array[i], thresh)==\"not_sure\":\n",
        "     #print(\"not sure\")\n",
        "     num_predicted_not_sure+=1\n",
        "     not_predict_indices=not_predict_indices+[i]\n",
        "    else:\n",
        "      y_predicted=y_predicted+ [label_or_not_sure(examples_array[i], thresh)]\n",
        "\n",
        "\n",
        "  cover=coverage(num_predicted_not_sure,len(examples_array))\n",
        "  \n",
        "  return cover, not_predict_indices, y_predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrIDFC-JyHrN",
        "colab_type": "text"
      },
      "source": [
        "### **Data Exploratory Analysis**\n",
        " \n",
        " **Data collection** step is already done as the dataset is given so we pass to the **Data Exploratory Analysis** to make sure that the data is already  **cleaned** and to explore it as well.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehweQZWveKcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "211252b7-9c20-43fd-a05e-19aeebe50d20"
      },
      "source": [
        "#Load data\n",
        "mnist=pd.read_csv('sample_data/mnist.csv')\n",
        "\n",
        "#print first five rows from mnist data\n",
        "print(mnist.head())\n",
        "\n",
        "#Verify if there is Null values\n",
        "print(mnist.info())\n",
        "\n",
        "print(mnist.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-befc98339150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmnist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/mnist.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print first five rows from mnist data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File sample_data/mnist.csv does not exist: 'sample_data/mnist.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PPD-a5BxxNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=mnist.drop(columns=['label'])\n",
        "y=mnist['label']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYCuGfTGSFAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print randomly one image from the csv file \n",
        "random_number=5\n",
        "\n",
        "img=np.array(X.iloc[random_number])\n",
        "img=img.reshape((28,28)) #28*28=784\n",
        "plt.imshow(img,cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(y.iloc[random_number])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYyD_YNWXiWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split data into train/test sets features and labels\n",
        "features_train, features_test, labels_train, labels_test= train_test_split(X,y,test_size=0.3, random_state=16, shuffle=True, stratify=y) \n",
        "\n",
        "#plot train labels each class(0-9) has at least 1000 examples\n",
        "plt.hist(labels_test, bins=10)\n",
        "plt.title(\"number of test examples for each class\")\n",
        "plt.ylabel(\"number of test examples\")\n",
        "plt.xlabel(\"class label\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhu81qmrm8_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshape, cast and scale training features  \n",
        "X_train=np.array(features_train).reshape(features_train.shape[0],28, 28,1).astype( 'float32' )/255.0\n",
        "\n",
        "#Reshape and Scale testing features  \n",
        "X_test=np.array(features_test).reshape(features_test.shape[0], 28, 28,1).astype( 'float32' )/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpUV5lyIN6LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set labels to one hot ecoder format\n",
        "labels_test= pd.get_dummies(labels_test)\n",
        "labels_train=pd.get_dummies(labels_train)\n",
        "\n",
        "#convert labels\n",
        "np.array(labels_train)\n",
        "np.array(labels_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkEP5b8NGG2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test GPU configuration\n",
        "tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.list_physical_devices(device_type=None)\n",
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3waBW670C2i",
        "colab_type": "text"
      },
      "source": [
        "### **Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXpBb_aR6SRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3), padding='same',activation='relu' ,input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "\n",
        "model.add(Conv2D(32,(3,3), input_shape=(28,28,1), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "\n",
        "model.add(AveragePooling2D(pool_size=(2)))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(256,(3,3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "model.add(Conv2D(256,(3,3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization(axis=3))\n",
        "\n",
        "\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_56_U03P9IZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng2UQ8upJHoV",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation and model fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceEKa44tyLNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create an image data generator uing affine transformation\n",
        "dataGen = ImageDataGenerator(rotation_range=15,width_shift_range=0.2,height_shift_range=0.2,\n",
        "                             shear_range=0.15,zoom_range=[0.5,2],validation_split=0.2)\n",
        "\n",
        "#fit the our existing training data into the generator\n",
        "dataGen.fit(X_train)\n",
        "\n",
        "#training data\n",
        "train_generator = dataGen.flow(X_train, labels_train, batch_size=64, shuffle=True, \n",
        "                               seed=2, save_to_dir=None, subset='training')\n",
        "#Vallidation data\n",
        "validation_generator = dataGen.flow(X_train, labels_train, batch_size=64, shuffle=True, \n",
        "                               seed=2, save_to_dir=None, subset='validation')\n",
        "\n",
        "#file path to save the model\n",
        "filepath_val_acc=\"model_gen_cnn_acc.best.hdf5\"\n",
        "\n",
        "# Model weights are saved at the end of every epoch, if it's the best seen\n",
        "# so far.\n",
        "checkpoint_val_acc = ModelCheckpoint(filepath_val_acc, monitor='acc', verbose=1,\n",
        "                                     save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint_val_acc]\n",
        "\n",
        "history_inception_datagen = model.fit_generator(train_generator,steps_per_epoch = 600,\n",
        "                                                validation_data = validation_generator,\n",
        "                                                validation_steps = 150, epochs=50,\n",
        "                                                callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrLdco_V8yeS",
        "colab_type": "text"
      },
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4mDQOyUfHvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predicted value from the test set\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "#convert one hot encoder test labels to one label between 0 and 9\n",
        "y_test=get_class_label_actual(labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcaB2HIWYs4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assign label to prediction according to the method1(highest existing probability)\n",
        "y_pred_method1= get_class_label_pred(y_pred)\n",
        "\n",
        "cnf_matrix= confusion_matrix(y_test, y_pred_method1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cnf_matrix)\n",
        "\n",
        "TPR,FPR= multi_class_rates(cnf_matrix)\n",
        "# Max FPR should be 0.00015\n",
        "print(\"FPR:\",FPR)\n",
        "print(\"TPR:\",TPR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW1vd4l3vXNf",
        "colab_type": "text"
      },
}
